{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BelayAbAb/Code/blob/Updated/eda_model_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYV91hbKwP2J"
      },
      "source": [
        "Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including [GPUs and TPUs](#using-accelerated-hardware), regardless of the power of your machine. All you need is a browser.\n",
        "\n",
        "For example, if you find yourself waiting for **pandas** code to finish running and want to go faster, you can switch to a GPU Runtime and use libraries like [RAPIDS cuDF](https://rapids.ai/cudf-pandas) that provide zero-code-change acceleration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary dependencies\n",
        "!pip install -q seaborn matplotlib scikit-learn gdown\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import gdown  # This will allow downloading from Google Drive\n",
        "\n",
        "# Step 3: Download the file using the shared Google Drive link\n",
        "\n",
        "# Shared link: https://drive.google.com/file/d/1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x/view?usp=sharing\n",
        "# Extract the file ID from the link (the ID is the part between /d/ and /view)\n",
        "file_id = '1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, 'data.csv', quiet=False)\n",
        "\n",
        "# Step 4: Load the CSV file from the local directory\n",
        "df = pd.read_csv('data.csv')  # The file will be downloaded to the current directory\n",
        "df.dataframeName = 'data.csv'\n",
        "\n",
        "# Step 5: Check the shape of the data\n",
        "nRow, nCol = df.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns in {df.dataframeName}')\n",
        "\n",
        "# Step 6: Take a quick look at the data\n",
        "print(df.head(5))\n",
        "\n",
        "# Step 7: Exploratory Data Analysis (EDA)\n",
        "\n",
        "# Filter out non-numeric columns for correlation and other numerical operations\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Define output directory in Google Drive\n",
        "output_dir = '/content/drive/MyDrive/1fINHoR_jYkPkHB-7HPm1fxQqIxeFOKnR/EDA_Results/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Distribution of numeric columns - Save the plot to Google Drive\n",
        "def plotPerColumnDistribution(df, nRows, nCols):\n",
        "    df.hist(figsize=(nRows, nCols))\n",
        "    plt.savefig(os.path.join(output_dir, 'distribution_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "plotPerColumnDistribution(numeric_df, 10, 5)\n",
        "\n",
        "# Correlation matrix of numeric columns - Save the plot to Google Drive\n",
        "def plotCorrelationMatrix(df, nRows):\n",
        "    corr = df.corr()\n",
        "    plt.figure(figsize=(nRows, nRows))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "    plt.savefig(os.path.join(output_dir, 'correlation_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "plotCorrelationMatrix(numeric_df, 8)\n",
        "\n",
        "# Scatter and density plots (only for numeric columns) - Save the plot to Google Drive\n",
        "def plotScatterMatrix(df, nRows, nCols):\n",
        "    sns.pairplot(df, height=2.5)\n",
        "    plt.savefig(os.path.join(output_dir, 'scatter_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "plotScatterMatrix(numeric_df, 12, 10)\n",
        "\n",
        "# Save the processed data as CSV to Google Drive\n",
        "output_csv_path = os.path.join(output_dir, 'processed_data.csv')\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# Conclusion message\n",
        "print(f\"All output files have been saved to: {output_dir}\")\n",
        "print(\"This concludes the exploratory data analysis!\")\n"
      ],
      "metadata": {
        "id": "zE4NZl-ik9wh",
        "outputId": "7714b8db-8440-468d-80d6-cd457c920e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x\n",
            "To: /content/data.csv\n",
            "100%|██████████| 17.4M/17.4M [00:00<00:00, 45.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 95662 rows and 16 columns in data.csv\n",
            "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
            "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
            "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
            "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
            "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
            "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
            "\n",
            "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
            "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
            "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
            "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
            "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
            "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
            "\n",
            "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
            "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
            "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
            "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
            "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
            "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
            "\n",
            "   PricingStrategy  FraudResult  \n",
            "0                2            0  \n",
            "1                2            0  \n",
            "2                2            0  \n",
            "3                2            0  \n",
            "4                2            0  \n",
            "All output files have been saved to: /content/drive/MyDrive/1fINHoR_jYkPkHB-7HPm1fxQqIxeFOKnR/EDA_Results/\n",
            "This concludes the exploratory data analysis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Download the dataset from Google Drive\n",
        "file_id = '1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "gdown.download(download_url, 'data.csv', quiet=False)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Check for missing values and data types\n",
        "print(\"Data types:\\n\", data.dtypes)\n",
        "print(\"Missing values:\\n\", data.isnull().sum())\n",
        "\n",
        "# Step 3: Split the data into features (X) and target (y)\n",
        "# We assume 'FraudResult' is the target variable for binary classification\n",
        "X = data.drop(columns=['FraudResult', 'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'TransactionStartTime'])\n",
        "y = data['FraudResult']\n",
        "\n",
        "# Step 4: Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 5: Define preprocessing steps for numerical and categorical features\n",
        "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(\"Numerical features:\", numerical_features)\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "\n",
        "# Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', StandardScaler(), numerical_features),\n",
        "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 6: Define models with increased max_iter for Logistic Regression\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=5000),  # Increased max_iter\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning parameters\n",
        "param_grid = {\n",
        "    'Logistic Regression': {\n",
        "        'model__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'model__solver': ['liblinear', 'lbfgs', 'saga', 'newton-cg']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [None, 10, 20, 30],\n",
        "        'model__min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__max_depth': [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 7: Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Create a pipeline with preprocessing and model\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('model', model)])\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    if model_name == 'Logistic Regression':\n",
        "        grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='f1', verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    else:\n",
        "        grid_search = RandomizedSearchCV(pipeline, param_grid[model_name], n_iter=10, cv=5, scoring='f1', random_state=42)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best model and its score\n",
        "    best_model = grid_search.best_estimator_\n",
        "    results[model_name] = {\n",
        "        'best_model': best_model,\n",
        "        'best_score': grid_search.best_score_,\n",
        "    }\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    results[model_name]['accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results[model_name]['precision'] = precision_score(y_test, y_pred)\n",
        "    results[model_name]['recall'] = recall_score(y_test, y_pred)\n",
        "    results[model_name]['f1_score'] = f1_score(y_test, y_pred)\n",
        "    results[model_name]['roc_auc'] = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    # Step 8: Confusion Matrix Visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Fraud', 'Fraud'])\n",
        "\n",
        "    # Save confusion matrix plot\n",
        "    output_folder = '/content/drive/MyDrive/your_folder_path_here/'\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    cm_output_path = os.path.join(output_folder, f\"cm_{model_name}.jpg\")\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.savefig(cm_output_path)\n",
        "    plt.close()\n",
        "\n",
        "# Step 9: Prepare data for plotting\n",
        "metric_names = ['Best CV Score', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
        "metric_values = {model: [results[model]['best_score'],\n",
        "                         results[model]['accuracy'],\n",
        "                         results[model]['precision'],\n",
        "                         results[model]['recall'],\n",
        "                         results[model]['f1_score'],\n",
        "                         results[model]['roc_auc']] for model in models.keys()}\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "width = 0.15  # Width of the bars\n",
        "x = np.arange(len(metric_names))  # the label locations\n",
        "\n",
        "for i, model in enumerate(models.keys()):\n",
        "    ax.bar(x + i * width, metric_values[model], width, label=model)\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Model Training and Evaluation Metrics')\n",
        "ax.set_xticks(x + width / 2)\n",
        "ax.set_xticklabels(metric_names)\n",
        "ax.legend()\n",
        "\n",
        "# Save the plot as JPG in the specified local folder\n",
        "output_plot_path = os.path.join(output_folder, \"model_evaluation_metrics.jpg\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_plot_path)\n",
        "plt.close()\n",
        "\n",
        "# Step 10: Save the results to a CSV file\n",
        "output_results_path = os.path.join(output_folder, \"model_results.csv\")\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "results_df.to_csv(output_results_path)\n",
        "\n",
        "print(\"Model evaluation metrics and results saved successfully.\")\n"
      ],
      "metadata": {
        "id": "_h4G3y0Ms7YF",
        "outputId": "739151fd-6316-4566-cd3b-4e8d0eebe9f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x\n",
            "To: /content/data.csv\n",
            "100%|██████████| 17.4M/17.4M [00:00<00:00, 41.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types:\n",
            " TransactionId            object\n",
            "BatchId                  object\n",
            "AccountId                object\n",
            "SubscriptionId           object\n",
            "CustomerId               object\n",
            "CurrencyCode             object\n",
            "CountryCode               int64\n",
            "ProviderId               object\n",
            "ProductId                object\n",
            "ProductCategory          object\n",
            "ChannelId                object\n",
            "Amount                  float64\n",
            "Value                     int64\n",
            "TransactionStartTime     object\n",
            "PricingStrategy           int64\n",
            "FraudResult               int64\n",
            "dtype: object\n",
            "Missing values:\n",
            " TransactionId           0\n",
            "BatchId                 0\n",
            "AccountId               0\n",
            "SubscriptionId          0\n",
            "CustomerId              0\n",
            "CurrencyCode            0\n",
            "CountryCode             0\n",
            "ProviderId              0\n",
            "ProductId               0\n",
            "ProductCategory         0\n",
            "ChannelId               0\n",
            "Amount                  0\n",
            "Value                   0\n",
            "TransactionStartTime    0\n",
            "PricingStrategy         0\n",
            "FraudResult             0\n",
            "dtype: int64\n",
            "Numerical features: ['CountryCode', 'Amount', 'Value', 'PricingStrategy']\n",
            "Categorical features: ['CurrencyCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId']\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Model evaluation metrics and results saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Mock dataset for testing (you should adapt it to your actual data)\n",
        "np.random.seed(42)\n",
        "df_cleaned = pd.DataFrame({\n",
        "    'ProductCategory': np.random.choice(['A', 'B', 'C', 'D'], size=1000),\n",
        "    'Recency': np.random.randint(1, 365, size=1000),  # Days since last purchase\n",
        "    'Frequency': np.random.randint(1, 50, size=1000),  # Number of transactions\n",
        "    'Monetary': np.random.randint(100, 5000, size=1000),  # Monetary value of transactions\n",
        "})\n",
        "\n",
        "# Mock binary target variable for demonstration (1 for good, 0 for bad)\n",
        "df_cleaned['Default'] = np.random.choice([0, 1], size=len(df_cleaned), p=[0.7, 0.3])\n",
        "\n",
        "# Function to calculate WoE for categorical features\n",
        "def calculate_woe(data, target, feature):\n",
        "    \"\"\"\n",
        "    Function to calculate Weight of Evidence (WoE) for categorical variables.\n",
        "\n",
        "    Caution: Ensure that the feature has enough data points per category.\n",
        "    Small sample sizes for some categories can lead to unreliable WoE values.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame for WoE calculation\n",
        "    woe_df = data.groupby(feature)[target].agg(['count', 'sum']).reset_index()\n",
        "    woe_df.columns = [feature, 'Total', 'Good']\n",
        "\n",
        "    # Calculate Bad\n",
        "    woe_df['Bad'] = woe_df['Total'] - woe_df['Good']\n",
        "\n",
        "    # Calculate proportions\n",
        "    total_good = woe_df['Good'].sum()\n",
        "    total_bad = woe_df['Bad'].sum()\n",
        "\n",
        "    # Calculate WoE\n",
        "    woe_df['Good_Percentage'] = woe_df['Good'] / total_good\n",
        "    woe_df['Bad_Percentage'] = woe_df['Bad'] / total_bad\n",
        "    woe_df['WoE'] = np.log(woe_df['Good_Percentage'] / woe_df['Bad_Percentage']).replace([-np.inf, np.inf], 0)\n",
        "\n",
        "    return woe_df[[feature, 'WoE']]\n",
        "\n",
        "# Function to apply WoE binning for continuous features (e.g., Recency, Frequency, Monetary)\n",
        "def binning_woe(data, target, feature, bins=5):\n",
        "    \"\"\"\n",
        "    Function to bin continuous features and calculate WoE for each bin.\n",
        "\n",
        "    Caution: Binning can be a very subjective process. The choice of the number of bins\n",
        "    and bin edges can significantly impact the results. Consider domain knowledge when\n",
        "    defining the binning strategy.\n",
        "    \"\"\"\n",
        "    # Bin continuous variable into intervals (e.g., Recency, Frequency, Monetary)\n",
        "    data['bin'] = pd.cut(data[feature], bins, right=False)\n",
        "\n",
        "    # Calculate WoE for each bin\n",
        "    return calculate_woe(data, target, 'bin')\n",
        "\n",
        "# Apply WoE Binning to 'ProductCategory' (categorical feature)\n",
        "woe_product_category = calculate_woe(df_cleaned, 'Default', 'ProductCategory')\n",
        "\n",
        "# Merge WoE values back to the original DataFrame\n",
        "df_cleaned = df_cleaned.merge(woe_product_category, on='ProductCategory', how='left')\n",
        "df_cleaned.rename(columns={'WoE': 'WoE_ProductCategory'}, inplace=True)\n",
        "\n",
        "# Apply WoE Binning to continuous features: 'Recency', 'Frequency', 'Monetary'\n",
        "woe_recency = binning_woe(df_cleaned, 'Default', 'Recency')\n",
        "woe_frequency = binning_woe(df_cleaned, 'Default', 'Frequency')\n",
        "woe_monetary = binning_woe(df_cleaned, 'Default', 'Monetary')\n",
        "\n",
        "# Merge WoE values for Recency, Frequency, and Monetary back to the DataFrame\n",
        "# Keep the 'bin' column for subsequent merges (no drop for the first two merges)\n",
        "df_cleaned = df_cleaned.merge(woe_recency, left_on='bin', right_on='bin', how='left')  # Do not drop 'bin' yet\n",
        "df_cleaned.rename(columns={'WoE': 'WoE_Recency'}, inplace=True)\n",
        "\n",
        "df_cleaned = df_cleaned.merge(woe_frequency, left_on='bin', right_on='bin', how='left')  # Do not drop 'bin' yet\n",
        "df_cleaned.rename(columns={'WoE': 'WoE_Frequency'}, inplace=True)\n",
        "\n",
        "# Now, drop 'bin' after the last merge for Monetary (the last step)\n",
        "df_cleaned = df_cleaned.merge(woe_monetary, left_on='bin', right_on='bin', how='left').drop('bin', axis=1)\n",
        "df_cleaned.rename(columns={'WoE': 'WoE_Monetary'}, inplace=True)\n",
        "\n",
        "# Display the first few rows of the modified DataFrame with WoE values\n",
        "print(df_cleaned[['ProductCategory', 'WoE_ProductCategory', 'Recency', 'WoE_Recency', 'Frequency', 'WoE_Frequency', 'Monetary', 'WoE_Monetary']].head())\n",
        "\n",
        "# Save the modified DataFrame with WoE values\n",
        "woe_output_path = r\"C:\\Users\\User\\Desktop\\woe_data.csv\"\n",
        "df_cleaned.to_csv(woe_output_path, index=False)\n",
        "\n",
        "print(f\"Woe data saved as {woe_output_path}\")\n",
        "\n",
        "# Step 2: Define a Default Estimator (proxy variable) based on the WoE values and RFMS\n",
        "# Create a composite score from the WoE values for Recency, Frequency, and Monetary\n",
        "df_cleaned['Risk_Score'] = df_cleaned['WoE_Recency'] + df_cleaned['WoE_Frequency'] + df_cleaned['WoE_Monetary']\n",
        "\n",
        "# Create a 'Risk' label: High risk (bad) if Risk_Score < threshold, Low risk (good) otherwise\n",
        "threshold = df_cleaned['Risk_Score'].median()  # Median can be a simple threshold, or use another strategy\n",
        "df_cleaned['Risk'] = np.where(df_cleaned['Risk_Score'] < threshold, 1, 0)  # 1 for high risk (bad), 0 for low risk (good)\n",
        "\n",
        "# Display the Risk segmentation\n",
        "print(df_cleaned[['Recency', 'Frequency', 'Monetary', 'Risk_Score', 'Risk']].head())\n",
        "\n",
        "# Additional caution about RFMS:\n",
        "# RFMS (Recency, Frequency, Monetary) is a common approach in marketing and credit scoring.\n",
        "# Ensure that your RFMS model aligns with industry best practices (e.g., Basel II Capital Accord)\n",
        "# and consult financial regulations to meet compliance.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hocNkJbvBSPj",
        "outputId": "458630bd-b317-471e-b89b-bdd540abbc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ProductCategory  WoE_ProductCategory  Recency  WoE_Recency  Frequency  \\\n",
            "0               C            -0.020074      145          NaN         29   \n",
            "1               D             0.091892      201          NaN         12   \n",
            "2               A            -0.059567      212          NaN         36   \n",
            "3               C            -0.020074      220          NaN         35   \n",
            "4               C            -0.020074      240          NaN         12   \n",
            "\n",
            "   WoE_Frequency  Monetary  WoE_Monetary  \n",
            "0            NaN      2330      0.072193  \n",
            "1            NaN      4140     -0.020298  \n",
            "2            NaN      4683     -0.020298  \n",
            "3            NaN      3797      0.043083  \n",
            "4            NaN      2937      0.072193  \n",
            "Woe data saved as C:\\Users\\User\\Desktop\\woe_data.csv\n",
            "   Recency  Frequency  Monetary  Risk_Score  Risk\n",
            "0      145         29      2330         NaN     0\n",
            "1      201         12      4140         NaN     0\n",
            "2      212         36      4683         NaN     0\n",
            "3      220         35      3797         NaN     0\n",
            "4      240         12      2937         NaN     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df_cleaned = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Split the data into features (X) and target (y)\n",
        "X = df_cleaned.drop(columns=['FraudResult', 'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'TransactionStartTime'])\n",
        "y = df_cleaned['FraudResult']\n",
        "\n",
        "# Step 3: Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Preprocessing\n",
        "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', StandardScaler(), numerical_features),\n",
        "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 5: Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=5000),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning parameters\n",
        "param_grid = {\n",
        "    'Logistic Regression': {\n",
        "        'model__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'model__solver': ['liblinear', 'lbfgs', 'saga', 'newton-cg']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [None, 10, 20, 30],\n",
        "        'model__min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__max_depth': [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 6: Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Create a pipeline with preprocessing and model\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('model', model)])\n",
        "\n",
        "    # Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
        "    if model_name == 'Logistic Regression':\n",
        "        grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='f1', verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    else:\n",
        "        grid_search = RandomizedSearchCV(pipeline, param_grid[model_name], n_iter=10, cv=5, scoring='f1', random_state=42)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best model and its score\n",
        "    best_model = grid_search.best_estimator_\n",
        "    results[model_name] = {\n",
        "        'best_model': best_model,\n",
        "        'best_score': grid_search.best_score_\n",
        "    }\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    results[model_name]['accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results[model_name]['precision'] = precision_score(y_test, y_pred)\n",
        "    results[model_name]['recall'] = recall_score(y_test, y_pred)\n",
        "    results[model_name]['f1_score'] = f1_score(y_test, y_pred)\n",
        "    results[model_name]['roc_auc'] = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    # Confusion Matrix Visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Fraud', 'Fraud'])\n",
        "\n",
        "    # Save confusion matrix plot\n",
        "    output_folder = '/content/drive/MyDrive/your_folder_path_here/'\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    cm_output_path = os.path.join(output_folder, f\"cm_{model_name}.jpg\")\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.savefig(cm_output_path)\n",
        "    plt.close()\n",
        "\n",
        "# Step 7: Risk Probability and Credit Score Mapping\n",
        "\n",
        "# Get the predicted probabilities for the test set\n",
        "y_pred_prob = best_model.predict_proba(X_test)[:, 1]  # Probability for the positive class (Fraud/High Risk)\n",
        "\n",
        "# Categorize customers into risk groups based on probabilities\n",
        "risk_groups = pd.cut(y_pred_prob, bins=[0, 0.2, 0.4, 0.6, 1.0], labels=['Very Low Risk', 'Low Risk', 'Moderate Risk', 'High Risk'])\n",
        "\n",
        "# Map risk groups to credit score\n",
        "# Assuming that higher risk corresponds to a lower credit score\n",
        "credit_score_mapping = {\n",
        "    'Very Low Risk': 800,  # Best credit score\n",
        "    'Low Risk': 700,\n",
        "    'Moderate Risk': 600,\n",
        "    'High Risk': 500  # Worst credit score\n",
        "}\n",
        "\n",
        "# Map the risk groups to credit scores\n",
        "credit_scores = risk_groups.map(credit_score_mapping)\n",
        "\n",
        "# Add the risk group and credit score columns to the dataframe\n",
        "df_cleaned.loc[X_test.index, 'Risk_Group'] = risk_groups\n",
        "df_cleaned.loc[X_test.index, 'Credit_Score'] = credit_scores\n",
        "\n",
        "# Display the first few rows of the data with risk groups and credit scores\n",
        "print(df_cleaned[['CustomerId', 'Risk_Group', 'Credit_Score']].head())\n",
        "\n",
        "# Save the data with risk groups and credit scores\n",
        "output_risk_path = os.path.join(output_folder, 'customer_risk_scores.csv')\n",
        "df_cleaned[['CustomerId', 'Risk_Group', 'Credit_Score']].to_csv(output_risk_path, index=False)\n",
        "\n",
        "print(f\"Customer risk data with credit scores saved at {output_risk_path}\")\n",
        "\n",
        "# Step 8: Plot the Risk Probability Distribution\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y_pred_prob, bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Risk Probability Distribution', fontsize=16)\n",
        "plt.xlabel('Risk Probability', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Save the histogram as a .jpg image\n",
        "output_hist_path = os.path.join(output_folder, 'risk_probability_distribution.jpg')\n",
        "plt.savefig(output_hist_path, format='jpg', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(f\"Risk probability distribution plot saved at {output_hist_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0tSnErbIhbA",
        "outputId": "7c3281dd-e9ff-42ae-f814-047f6009babb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "        CustomerId     Risk_Group Credit_Score\n",
            "0  CustomerId_4406            NaN          NaN\n",
            "1  CustomerId_4406            NaN          NaN\n",
            "2  CustomerId_4683  Very Low Risk          800\n",
            "3   CustomerId_988            NaN          NaN\n",
            "4   CustomerId_988            NaN          NaN\n",
            "Customer risk data with credit scores saved at /content/drive/MyDrive/your_folder_path_here/customer_risk_scores.csv\n",
            "Risk probability distribution plot saved at /content/drive/MyDrive/your_folder_path_here/risk_probability_distribution.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# Modify the path to your actual file location\n",
        "\n",
        "# If you're using Google Colab or Jupyter with Google Drive:\n",
        "df_cleaned = pd.read_csv('/content/drive/MyDrive/your_folder_path_here/data.csv')\n",
        "\n",
        "# If the file is in the same directory as your script, use:\n",
        "# df_cleaned = pd.read_csv('data.csv')\n",
        "\n",
        "# Alternatively, you can specify the full path of the dataset if it's in a different folder:\n",
        "# df_cleaned = pd.read_csv('/path/to/your/folder/data.csv')\n",
        "\n",
        "# Step 2: Split the data into features (X) and target (y)\n",
        "X = df_cleaned.drop(columns=['FraudResult', 'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'TransactionStartTime'])\n",
        "y = df_cleaned['FraudResult']\n",
        "\n",
        "# Step 3: Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Preprocessing\n",
        "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', StandardScaler(), numerical_features),\n",
        "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 5: Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=5000),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning parameters\n",
        "param_grid = {\n",
        "    'Logistic Regression': {\n",
        "        'model__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'model__solver': ['liblinear', 'lbfgs', 'saga', 'newton-cg']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [None, 10, 20, 30],\n",
        "        'model__min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__max_depth': [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 6: Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Create a pipeline with preprocessing and model\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('model', model)])\n",
        "\n",
        "    # Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
        "    if model_name == 'Logistic Regression':\n",
        "        grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='f1', verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    else:\n",
        "        grid_search = RandomizedSearchCV(pipeline, param_grid[model_name], n_iter=10, cv=5, scoring='f1', random_state=42)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best model and its score\n",
        "    best_model = grid_search.best_estimator_\n",
        "    results[model_name] = {\n",
        "        'best_model': best_model,\n",
        "        'best_score': grid_search.best_score_\n",
        "    }\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    results[model_name]['accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results[model_name]['precision'] = precision_score(y_test, y_pred)\n",
        "    results[model_name]['recall'] = recall_score(y_test, y_pred)\n",
        "    results[model_name]['f1_score'] = f1_score(y_test, y_pred)\n",
        "    results[model_name]['roc_auc'] = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    # Confusion Matrix Visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Fraud', 'Fraud'])\n",
        "\n",
        "    # Save confusion matrix plot\n",
        "    output_folder = '/content/drive/MyDrive/your_folder_path_here/'\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    cm_output_path = os.path.join(output_folder, f\"cm_{model_name}.jpg\")\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.savefig(cm_output_path)\n",
        "    plt.close()\n",
        "\n",
        "# Step 7: Risk Probability and Credit Score Mapping\n",
        "\n",
        "# Get the predicted probabilities for the test set\n",
        "best_model = results['Random Forest']['best_model']  # Assume RandomForest performs best, adjust if needed\n",
        "y_pred_prob = best_model.predict_proba(X_test)[:, 1]  # Probability for the positive class (Fraud/High Risk)\n",
        "\n",
        "# Categorize customers into risk groups based on probabilities\n",
        "risk_groups = pd.cut(y_pred_prob, bins=[0, 0.2, 0.4, 0.6, 1.0], labels=['Very Low Risk', 'Low Risk', 'Moderate Risk', 'High Risk'])\n",
        "\n",
        "# Map risk groups to credit score\n",
        "credit_score_mapping = {\n",
        "    'Very Low Risk': 800,  # Best credit score\n",
        "    'Low Risk': 700,\n",
        "    'Moderate Risk': 600,\n",
        "    'High Risk': 500  # Worst credit score\n",
        "}\n",
        "\n",
        "# Map the risk groups to credit scores\n",
        "credit_scores = risk_groups.map(credit_score_mapping)\n",
        "\n",
        "# Add the risk group and credit score columns to the dataframe\n",
        "df_cleaned.loc[X_test.index, 'Risk_Group'] = risk_groups\n",
        "df_cleaned.loc[X_test.index, 'Credit_Score'] = credit_scores\n",
        "\n",
        "# Display the first few rows of the data with risk groups and credit scores\n",
        "print(df_cleaned[['CustomerId', 'Risk_Group', 'Credit_Score']].head())\n",
        "\n",
        "# Save the data with risk groups and credit scores\n",
        "output_risk_path = os.path.join(output_folder, 'customer_risk_scores.csv')\n",
        "df_cleaned[['CustomerId', 'Risk_Group', 'Credit_Score']].to_csv(output_risk_path, index=False)\n",
        "\n",
        "print(f\"Customer risk data with credit scores saved at {output_risk_path}\")\n",
        "\n",
        "\n",
        "# Step 8: Summarize the output: Risk Groups and Statistics\n",
        "\n",
        "# Group by risk group and summarize\n",
        "risk_summary = df_cleaned[['CustomerId', 'Risk_Group', 'Credit_Score']].groupby('Risk_Group').agg(\n",
        "    count=('CustomerId', 'count'),\n",
        "    # Convert 'Credit_Score' to numeric before calculating the mean to avoid errors\n",
        "    avg_credit_score=('Credit_Score', lambda x: pd.to_numeric(x, errors='coerce').mean()),\n",
        "    min_credit_score=('Credit_Score', 'min'),\n",
        "    max_credit_score=('Credit_Score', 'max'),\n",
        "    risk_probability_range=('Credit_Score', lambda x: (x.min(), x.max()))\n",
        ").reset_index()\n",
        "\n",
        "# Display the risk group summary\n",
        "print(\"\\nRisk Group Summary:\")\n",
        "print(risk_summary)\n",
        "\n",
        "# Optionally, you can also save this summary to a CSV file for further use\n",
        "output_summary_path = '/content/drive/MyDrive/your_folder_path_here/risk_group_summary.csv'\n",
        "risk_summary.to_csv(output_summary_path, index=False)\n",
        "\n",
        "print(f\"Risk group summary saved at {output_summary_path}\")\n",
        "\n",
        "# ... (rest of the code remains the same) ...\n",
        "\n",
        "\n",
        "# Step 9: Plot the Risk Probability Distribution\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y_pred_prob, bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Risk Probability Distribution', fontsize=16)\n",
        "plt.xlabel('Risk Probability', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Save the histogram as a .jpg image\n",
        "output_hist_path = os.path.join(output_folder, 'risk_probability_distribution.jpg')\n",
        "plt.savefig(output_hist_path, format='jpg', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(f\"Risk probability distribution plot saved at {output_hist_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBfnaLsnQXhQ",
        "outputId": "d08f8d6b-dd1d-4292-e388-6217388c1448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/your_folder_path_here/data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-321860f94f2a>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# If you're using Google Colab or Jupyter with Google Drive:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/your_folder_path_here/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# If the file is in the same directory as your script, use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/your_folder_path_here/data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OS4KcEWcMCRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Sample Data (replace this with your actual dataset)\n",
        "# Assuming 'df_cleaned' is your dataset and 'FraudResult' is your target variable\n",
        "# X = df_cleaned.drop(columns=['FraudResult', 'other_columns'])\n",
        "# y = df_cleaned['FraudResult']\n",
        "\n",
        "# For illustration, I'll use dummy data (replace it with your actual data)\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Step 1: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 2: Create a model pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Optional preprocessing step (scaling)\n",
        "    ('model', RandomForestClassifier(random_state=42))  # Model\n",
        "])\n",
        "\n",
        "# Step 3: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model to a .joblib file\n",
        "joblib.dump(pipeline, 'credit_risk_model.joblib')\n",
        "\n",
        "print(\"Model saved as 'credit_risk_model.joblib'\")\n",
        "\n",
        "# Optional: Test loading the model and making predictions\n",
        "loaded_model = joblib.load('credit_risk_model.joblib')\n",
        "\n",
        "# Example: Use the loaded model to make predictions on the test set\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Print some predictions\n",
        "print(\"Predictions on test set:\", y_pred[:10])  # Print first 10 predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0cF9i3bCUo3",
        "outputId": "eee20ebb-1fec-4b30-bd2d-cb9be81e9345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'credit_risk_model.joblib'\n",
            "Predictions on test set: [0 1 0 1 0 1 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6HnmqeCb_RI",
        "outputId": "13a43677-5b60-43a7-d8b5-bc5ccbf5d0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.5 starlette-0.41.3\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.12.2)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn\n",
            "Successfully installed uvicorn-0.32.0\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import streamlit as st\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the entire model pipeline (ensure the model file is present in the working directory)\n",
        "model = joblib.load(\"credit_risk_model.joblib\")\n",
        "\n",
        "# Function to categorize risk group based on probability\n",
        "def categorize_risk(probability: float) -> str:\n",
        "    if probability >= 0.8:\n",
        "        return \"High Risk\"\n",
        "    elif 0.6 <= probability < 0.8:\n",
        "        return \"Moderate Risk\"\n",
        "    elif 0.4 <= probability < 0.6:\n",
        "        return \"Low Risk\"\n",
        "    else:\n",
        "        return \"Very Low Risk\"\n",
        "\n",
        "# Function to handle the product category encoding\n",
        "def encode_product_category(product_category: str) -> int:\n",
        "    # Mapping product categories to integers (you can customize this based on your model's encoding)\n",
        "    product_mapping = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "    return product_mapping.get(product_category, -1)  # Return -1 if invalid category\n",
        "\n",
        "# Define the Streamlit UI\n",
        "def main():\n",
        "    # Set the title of the app\n",
        "    st.title(\"AI-Powered Credit Scoring Model\")\n",
        "\n",
        "    # Define input form for customer features\n",
        "    recency = st.number_input(\"Recency\", min_value=0.0, step=0.1)\n",
        "    frequency = st.number_input(\"Frequency\", min_value=0.0, step=0.1)\n",
        "    monetary = st.number_input(\"Monetary\", min_value=0.0, step=0.1)\n",
        "    product_category = st.selectbox(\"Product Category\", [\"A\", \"B\", \"C\", \"D\"])  # Assuming a few product categories\n",
        "\n",
        "    # Button to run prediction\n",
        "    if st.button(\"Predict Credit Risk\"):\n",
        "        # Encode the product category\n",
        "        encoded_product_category = encode_product_category(product_category)\n",
        "\n",
        "        # Ensure that the encoded value is valid (you can improve this with error messages)\n",
        "        if encoded_product_category == -1:\n",
        "            st.error(\"Invalid product category selected.\")\n",
        "            return\n",
        "\n",
        "        # Prepare customer data for prediction (ensure it's in a DataFrame)\n",
        "        customer_data = pd.DataFrame({\n",
        "            \"recency\": [recency],\n",
        "            \"frequency\": [frequency],\n",
        "            \"monetary\": [monetary],\n",
        "            \"product_category\": [encoded_product_category]  # Now using encoded value\n",
        "        })\n",
        "\n",
        "        # Check if customer_data looks correct before passing to the model\n",
        "        st.write(\"Customer Data:\", customer_data)\n",
        "\n",
        "        # Preprocessing and prediction\n",
        "        try:\n",
        "            # Use the entire pipeline (if the model is wrapped in one) to preprocess the data\n",
        "            prediction = model.predict_proba(customer_data)[:, 1]  # Assuming binary classification\n",
        "            risk_probability = prediction[0]\n",
        "            risk_group = categorize_risk(risk_probability)\n",
        "            credit_score = risk_probability * 800  # Adjust scale as needed\n",
        "            loan_recommendation = \"Loan Denied\" if risk_group == \"High Risk\" else \"Loan Approved\"\n",
        "\n",
        "            # Display results\n",
        "            st.subheader(\"Prediction Results\")\n",
        "            st.write(f\"**Risk Group**: {risk_group}\")\n",
        "            st.write(f\"**Credit Score**: {credit_score:.2f}\")\n",
        "            st.write(f\"**Risk Probability**: {risk_probability:.2f}\")\n",
        "            st.write(f\"**Loan Recommendation**: {loan_recommendation}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during prediction: {str(e)}\")\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAW_j3ImUx97",
        "outputId": "9af21ee7-1f30-4e51-abd1-8d02b2220b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-19 06:17:17.848 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.450 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-11-19 06:17:18.459 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.471 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.481 Session state does not function when running a script without `streamlit run`\n",
            "2024-11-19 06:17:18.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.492 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.494 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.545 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.607 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.611 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.637 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-19 06:17:18.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQNAD4D-D44x",
        "outputId": "dd4a9fa2-18fd-48fe-fa16-6e5f90bfcf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep-bqqU1U-O5",
        "outputId": "43dda45b-25f7-48c7-cf7f-d25a212a23e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW3wO0xgVREn",
        "outputId": "33f26e0b-e047-4c13-970f-6c133516244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3082, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3135, in __init__\n",
            "    super().__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 47, in __init__\n",
            "    self.marker._markers = _normalize_extra_values(parsed.marker)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 128, in _normalize_extra_values\n",
            "    normalized_extra = canonicalize_name(rhs.value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/utils.py\", line 49, in canonicalize_name\n",
            "    value = _canonicalize_regex.sub(\"-\", name).lower()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 168, in emit\n",
            "    message = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 113, in format\n",
            "    message_start = self.get_message_start(formatted, record.levelno)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 98, in get_message_start\n",
            "    if formatted.startswith(DEPRECATION_MSG_PREFIX):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok # Install the pyngrok library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25q--2kEnxp",
        "outputId": "e70daa2d-513e-4df3-a84b-d1a83d2cbe5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Using cached pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Using cached pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}